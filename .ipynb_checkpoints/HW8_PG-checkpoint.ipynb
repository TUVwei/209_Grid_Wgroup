{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "import torch\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_map = [[0]*5 for _ in range(5)]\n",
    "my_map[2][0] = 'Rs'\n",
    "my_map[2][2] = 'Rd'\n",
    "my_map[1][1] = -1\n",
    "my_map[2][1] = -1\n",
    "my_map[1][3] = -1\n",
    "my_map[2][3] = -1\n",
    "my_map[4][0] = 'Rw'\n",
    "my_map[4][1] = 'Rw'\n",
    "my_map[4][2] = 'Rw'\n",
    "my_map[4][3] = 'Rw'\n",
    "my_map[4][4] = 'Rw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function plots a map with various ways\n",
    "# given my_map, it plots the map: plot_map(my_map)\n",
    "# given my_map, positions (trajectory)ï¼Œ it will plot it on the map: plot_map(my_map, [[0,0], [0,1], [0,2], [1,2]] )\n",
    "# given my_map, positions, observations, it will plot the trajectory with the oservation: plot_map(my_map, [[0,0], [0,1], [0,2], [1,2]], [1,2,3,4])\n",
    "# given my_map, values (same size with map), it will plot the values. This can be used to show action/value\n",
    "def plot_map(my_map, positions=[], observations=[], values=[]):\n",
    "    # draw the grid frame\n",
    "    n = len(my_map)\n",
    "    m = len(my_map[0])\n",
    "    fig,ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # draw the whole grid\n",
    "    for x,row in enumerate(my_map):\n",
    "        for y,element in enumerate(row):\n",
    "            if type(element)==type(0) and element == -1:\n",
    "                ax.add_patch(Rectangle((x,y),1,1,color='lightgray'))\n",
    "                ax.text(x+0.4,y+0.4,str('X'),c='black',size='xx-large')\n",
    "            elif type(element)==type('str'):\n",
    "                if element == 'Rw':\n",
    "                    ax.add_patch(Rectangle((x,y),1,1,color='red'))\n",
    "                    if len(values) == 0:\n",
    "                        ax.text(x+0.35,y+0.4,element,c='black',size='xx-large')\n",
    "                else:\n",
    "                    ax.add_patch(Rectangle((x,y),1,1,color='lime'))\n",
    "                    if len(values) == 0:\n",
    "                        ax.text(x+0.35,y+0.4,element,c='black',size='xx-large')                   \n",
    "    \n",
    "    # if need rewards\n",
    "    if len(values) > 0:\n",
    "        for x,row in enumerate(my_map):\n",
    "            for y,element in enumerate(row):\n",
    "                if type(element)==type(0) and element == -1:\n",
    "                    pass\n",
    "                else:\n",
    "                    ax.text(x+0.15,y+0.4,'{:.2f}'.format(values[x][y]),c='black',size='xx-large')\n",
    "    else:\n",
    "        # draw arrows of start\n",
    "        if len(positions) > 0:\n",
    "            ax.add_patch(Circle((positions[0][0]+0.5, positions[0][1]+0.5), 0.05, color='blue'))\n",
    "            ax.text(positions[0][0]+0.05, positions[0][1]+0.2,'0',c='black',size='small') \n",
    "            if len(observations) > 0:\n",
    "                ax.text(positions[0][0]+0.05, positions[0][1]+0.7, str(observations[0]),c='gray',size='small') \n",
    "\n",
    "        # draw all the movements\n",
    "        max_num = len(positions)\n",
    "        for num, pos in enumerate(positions[1:]):\n",
    "            ax.add_patch(Circle((pos[0]+0.5, pos[1]+0.5), 0.05+0.15*(num+1)/max_num, color='blue'))\n",
    "            ax.text(pos[0]+0.05+0.9*(num+1)/max_num, pos[1]+0.2, str(num+1), c='black', size='small') \n",
    "            if len(observations) == max_num:\n",
    "                ax.text(pos[0]+0.05+0.9*(num+1)/max_num, pos[1]+0.7, str(observations[num+1]),c='gray',size='small') \n",
    "    \n",
    "    # some configs\n",
    "    plt.xticks(np.arange(n+1))\n",
    "    plt.yticks(np.arange(m+1))\n",
    "\n",
    "    ax.set_xticklabels([str(x) for x in np.arange(n+1)])\n",
    "    ax.set_yticklabels([str(y) for y in np.arange(m+1)])\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_map(my_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves_map = {0:[0,0], 1:[-1,0], 2:[0,1], 3:[1,0], 4:[0,-1]}\n",
    "# this function receive a position and action and return next position with correct randomness\n",
    "def move_one_step(my_map, pos, move, pe, n=5, m=5):\n",
    "    # if already stop/ never\n",
    "    #if type(my_map[pos[0]][pos[1]]) == type('str'):\n",
    "    #    return pos\n",
    "    \n",
    "    # else choose the random action\n",
    "    actions = [(1-pe)/(len(moves_map)-1)] * len(moves_map)\n",
    "    actions[move] = pe\n",
    "    action = np.argmax(np.random.multinomial(1, actions))\n",
    "    movement = moves_map[action]\n",
    "    # next pois\n",
    "    next_pos =[pos[0]+movement[0], pos[1]+movement[1]]\n",
    "    \n",
    "    # if not valid: out the map\n",
    "    if next_pos[0] < 0 or next_pos[0] >= n:\n",
    "        return pos\n",
    "    if next_pos[1] < 0 or next_pos[1] >= m:\n",
    "        return pos\n",
    "    \n",
    "    # else if target is obstacle\n",
    "    if type(my_map[next_pos[0]][next_pos[1]]) == type(0) and my_map[next_pos[0]][next_pos[1]] == -1:\n",
    "        return pos\n",
    "    \n",
    "    return next_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from http://quant.am/cs/2017/08/07/policy-gradients/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "class LinearSoftmaxAgent():\n",
    "    \"\"\"Act with softmax policy. Features are encoded as\n",
    "    phi(s, a) is a 1-hot vector of states.\"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.rewards = []\n",
    "        self.theta = np.random.random(state_size * action_size)\n",
    "        self.alpha = .01\n",
    "        self.gamma = .99\n",
    "\n",
    "    def store(self, state, action, prob, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(prob)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def _phi(self, s, a):\n",
    "        encoded = np.zeros([self.action_size, self.state_size])\n",
    "        encoded[a] = s\n",
    "        return encoded.flatten()\n",
    "\n",
    "    def _softmax(self, s, a):\n",
    "        return np.exp(self.theta.dot(self._phi(s, a)) / 100)\n",
    "\n",
    "    def pi(self, s):\n",
    "        \"\"\"\\pi(a | s)\"\"\"\n",
    "        weights = np.empty(self.action_size)\n",
    "        for a in range(self.action_size):\n",
    "            weights[a] = self._softmax(s, a)\n",
    "        return weights / np.sum(weights)\n",
    "\n",
    "    def act(self, state):\n",
    "        probs = self.pi(state)\n",
    "        a = random.choices(range(0, self.action_size), weights=probs)\n",
    "        a = a[0]\n",
    "        pi = probs[a]\n",
    "        return (a, pi)\n",
    "\n",
    "    def _gradient(self, s, a):\n",
    "        expected = 0\n",
    "        probs = self.pi(s)\n",
    "        for b in range(0, self.action_size):\n",
    "            expected += probs[b] * self._phi(s, b)\n",
    "        return self._phi(s, a) - expected\n",
    "\n",
    "    def _R(self, t):\n",
    "        \"\"\"Reward function.\"\"\"\n",
    "        total = 0\n",
    "        for tau in range(t, len(self.rewards)):\n",
    "            total += self.gamma**(tau - t) * self.rewards[tau]\n",
    "        return total\n",
    "\n",
    "    def train(self):\n",
    "        self.rewards -= np.mean(self.rewards)\n",
    "        self.rewards /= np.std(self.rewards)\n",
    "        for t in range(len(self.states)):\n",
    "            s = self.states[t]\n",
    "            a = self.actions[t]\n",
    "            r = self._R(t)\n",
    "            grad = self._gradient(s, a)\n",
    "            self.theta = self.theta + self.alpha * r * grad\n",
    "        # print(self.theta)\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def getName(self):\n",
    "        return 'LinearSoftmaxAgent'\n",
    "\n",
    "    def save(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = 0.9\n",
    "H = 30\n",
    "discount = 0.9\n",
    "rewards_map = {'Rd' : 10, 'Rs': 10, 'Rw': -5, 0:-0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Score: 3.100000000000005\n",
      "Episode: 200 Score: 7.700000000000003\n",
      "Episode: 300 Score: -16.800000000000004\n",
      "Episode: 400 Score: 12.900000000000002\n",
      "Episode: 500 Score: -32.10000000000001\n",
      "Episode: 600 Score: 3.1000000000000085\n",
      "Episode: 700 Score: -22.299999999999997\n",
      "Episode: 800 Score: -6.999999999999996\n",
      "Episode: 900 Score: -2.1\n",
      "Episode: 1000 Score: -16.8\n",
      "Episode: 1100 Score: -2.0999999999999974\n",
      "Episode: 1200 Score: -56.90000000000002\n",
      "Episode: 1300 Score: -17.100000000000012\n",
      "Episode: 1400 Score: -22.000000000000004\n",
      "Episode: 1500 Score: 8.000000000000002\n",
      "Episode: 1600 Score: -16.8\n",
      "Episode: 1700 Score: -37.30000000000002\n",
      "Episode: 1800 Score: -27.200000000000017\n",
      "Episode: 1900 Score: 2.800000000000004\n",
      "Episode: 2000 Score: -51.700000000000024\n",
      "Episode: 2100 Score: -31.800000000000004\n",
      "Episode: 2200 Score: -22.00000000000001\n",
      "Episode: 2300 Score: -2.1\n",
      "Episode: 2400 Score: -22.0\n",
      "Episode: 2500 Score: -7.0\n",
      "Episode: 2600 Score: -2.100000000000005\n",
      "Episode: 2700 Score: 8.299999999999992\n",
      "Episode: 2800 Score: -51.70000000000001\n",
      "Episode: 2900 Score: -2.0999999999999988\n",
      "Episode: 3000 Score: -11.6\n",
      "Episode: 3100 Score: -31.80000000000002\n",
      "Episode: 3200 Score: 23.299999999999994\n",
      "Episode: 3300 Score: 13.200000000000005\n",
      "Episode: 3400 Score: 7.999999999999998\n",
      "Episode: 3500 Score: -22.000000000000014\n",
      "Episode: 3600 Score: -17.400000000000016\n",
      "Episode: 3700 Score: -46.80000000000002\n",
      "Episode: 3800 Score: -22.299999999999997\n",
      "Episode: 3900 Score: -22.000000000000014\n",
      "Episode: 4000 Score: -12.2\n",
      "Episode: 4100 Score: -41.300000000000004\n",
      "Episode: 4200 Score: -2.099999999999995\n",
      "Episode: 4300 Score: 8.600000000000001\n",
      "Episode: 4400 Score: -32.100000000000016\n",
      "Episode: 4500 Score: 48.099999999999994\n",
      "Episode: 4600 Score: -27.200000000000017\n",
      "Episode: 4700 Score: 3.100000000000006\n",
      "Episode: 4800 Score: 18.099999999999984\n",
      "Episode: 4900 Score: 2.800000000000002\n",
      "Episode: 5000 Score: 8.000000000000002\n",
      "Episode: 5100 Score: 2.4999999999999996\n",
      "Episode: 5200 Score: 27.89999999999997\n",
      "Episode: 5300 Score: 23.0\n",
      "Episode: 5400 Score: -27.200000000000006\n",
      "Episode: 5500 Score: 7.999999999999993\n",
      "Episode: 5600 Score: -27.500000000000018\n",
      "Episode: 5700 Score: 17.800000000000004\n",
      "Episode: 5800 Score: 13.199999999999996\n",
      "Episode: 5900 Score: -2.400000000000005\n",
      "Episode: 6000 Score: -32.10000000000001\n",
      "Episode: 6100 Score: -7.0000000000000036\n",
      "Episode: 6200 Score: 28.19999999999998\n",
      "Episode: 6300 Score: 12.899999999999991\n",
      "Episode: 6400 Score: 18.4\n",
      "Episode: 6500 Score: 2.800000000000003\n",
      "Episode: 6600 Score: -27.200000000000006\n",
      "Episode: 6700 Score: -2.100000000000008\n",
      "Episode: 6800 Score: -26.599999999999998\n",
      "Episode: 6900 Score: -21.4\n",
      "Episode: 7000 Score: -22.30000000000001\n",
      "Episode: 7100 Score: -6.999999999999996\n",
      "Episode: 7200 Score: -1.4999999999999982\n",
      "Episode: 7300 Score: -17.10000000000001\n",
      "Episode: 7400 Score: -37.000000000000014\n",
      "Episode: 7500 Score: -22.60000000000001\n",
      "Episode: 7600 Score: -32.10000000000001\n",
      "Episode: 7700 Score: 3.4000000000000052\n",
      "Episode: 7800 Score: -41.900000000000006\n",
      "Episode: 7900 Score: 33.099999999999994\n",
      "Episode: 8000 Score: -12.199999999999996\n",
      "Episode: 8100 Score: -36.7\n",
      "Episode: 8200 Score: -2.0999999999999956\n",
      "Episode: 8300 Score: -7.299999999999997\n",
      "Episode: 8400 Score: -2.099999999999998\n",
      "Episode: 8500 Score: -11.900000000000007\n",
      "Episode: 8600 Score: 18.099999999999994\n",
      "Episode: 8700 Score: 27.9\n",
      "Episode: 8800 Score: -2.1000000000000028\n",
      "Episode: 8900 Score: -11.9\n",
      "Episode: 9000 Score: -17.100000000000012\n",
      "Episode: 9100 Score: -56.90000000000001\n",
      "Episode: 9200 Score: -36.70000000000001\n",
      "Episode: 9300 Score: -27.200000000000003\n",
      "Episode: 9400 Score: 2.5000000000000004\n",
      "Episode: 9500 Score: 22.999999999999993\n",
      "Episode: 9600 Score: -6.9999999999999964\n",
      "Episode: 9700 Score: 3.100000000000003\n",
      "Episode: 9800 Score: -17.10000000000001\n",
      "Episode: 9900 Score: -31.80000000000001\n",
      "Episode: 10000 Score: -6.9999999999999964\n"
     ]
    }
   ],
   "source": [
    "g = LinearSoftmaxAgent(25, 5)\n",
    "\n",
    "MAX_EPISODES = 10000\n",
    "h = 0\n",
    "score = 0\n",
    "episode=0\n",
    "while episode < MAX_EPISODES:  # episode loop\n",
    "    # get state\n",
    "    state = [np.random.randint(0,5), np.random.randint(0,5)]\n",
    "    tmpre = my_map[state[0]][state[1]]\n",
    "    while type(tmpre) == type(0) and tmpre == -1:\n",
    "        state = [np.random.randint(0,5), np.random.randint(0,5)]\n",
    "        tmpre = my_map[state[0]][state[1]]\n",
    "        \n",
    "    action, prob = g.act(state[0]*5+state[1])\n",
    "    state_next = move_one_step(my_map, state, action, pe)  # take a random action\n",
    "    reward = rewards_map[my_map[state[0]][state[1]]]\n",
    "    score += reward\n",
    "    g.store(state[0]*5 + state[1], action, prob, reward)\n",
    "    \n",
    "    h+=1\n",
    "    if h>=30:\n",
    "        episode += 1\n",
    "        g.train()\n",
    "        if episode%100 == 0:\n",
    "            print('Episode: {} Score: {}'.format(episode, score))\n",
    "        score = 0\n",
    "        h = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(my_map, track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = 0.9\n",
    "H = 30\n",
    "discount = 0.9\n",
    "rewards_map = {'Rd' : 10, 'Rs': 10, 'Rw': -5, 0:-0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Myenv():\n",
    "    def __init__(self, my_map, rewards_map, pe, H):\n",
    "        self.my_map = my_map\n",
    "        self.rewards_map = rewards_map\n",
    "        self.pe = pe\n",
    "        self.H = H\n",
    "        self.h = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        state = [np.random.randint(0,5), np.random.randint(0,5)]\n",
    "        tmpre = my_map[state[0]][state[1]]\n",
    "        while type(tmpre) == type(0) and tmpre == -1:\n",
    "            state = [np.random.randint(0,5), np.random.randint(0,5)]\n",
    "            tmpre = my_map[state[0]][state[1]]\n",
    "        s = np.hstack([np.zeros([1,10]), np.ones([1,10])])\n",
    "        s[0, 0 + state[0]] = 1.0\n",
    "        s[0, 5 + state[1]] = 1.0\n",
    "        s[0,10 + state[0]] = 0.0\n",
    "        s[0,15 + state[1]] = 0.0\n",
    "        \n",
    "        self.state = state\n",
    "        \n",
    "        self.h = 0\n",
    "        return s\n",
    "    \n",
    "    def step(self, action):\n",
    "        state = move_one_step(self.my_map, self.state, action, self.pe, n=5, m=5)\n",
    "        r = self.rewards_map[self.my_map[state[0]][state[1]]]\n",
    "        \n",
    "        s = np.hstack([np.zeros([1,10]), np.ones([1,10])])\n",
    "        s[0, 0 + state[0]] = 1.0\n",
    "        s[0, 5 + state[1]] = 1.0\n",
    "        s[0,10 + state[0]] = 0.0\n",
    "        s[0,15 + state[1]] = 0.0\n",
    "        \n",
    "        self.state = state\n",
    "        self.h += 1\n",
    "        done = False\n",
    "        if self.h >= self.H:\n",
    "            done = True\n",
    "        \n",
    "        return s, r, done, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast reward: -3.00\tAverage reward: -14.63\n",
      "Episode 200\tLast reward: 47.50\tAverage reward: -8.97\n",
      "Episode 300\tLast reward: -30.60\tAverage reward: -19.44\n",
      "Episode 400\tLast reward: -90.60\tAverage reward: -15.05\n",
      "Episode 500\tLast reward: -3.00\tAverage reward: -19.84\n",
      "Episode 600\tLast reward: -90.90\tAverage reward: -21.45\n",
      "Episode 700\tLast reward: -3.00\tAverage reward: -10.26\n",
      "Episode 800\tLast reward: -76.50\tAverage reward: -1.82\n",
      "Episode 900\tLast reward: 42.60\tAverage reward: 4.57\n",
      "Episode 1000\tLast reward: -3.00\tAverage reward: -0.56\n",
      "Episode 1100\tLast reward: -66.40\tAverage reward: -25.96\n",
      "Episode 1200\tLast reward: -41.30\tAverage reward: -4.92\n",
      "Episode 1300\tLast reward: 48.10\tAverage reward: -5.41\n",
      "Episode 1400\tLast reward: -3.00\tAverage reward: 2.39\n",
      "Episode 1500\tLast reward: -3.00\tAverage reward: -6.97\n",
      "Episode 1600\tLast reward: -91.20\tAverage reward: -9.87\n",
      "Episode 1700\tLast reward: -30.90\tAverage reward: -2.03\n",
      "Episode 1800\tLast reward: -7.00\tAverage reward: -14.48\n",
      "Episode 1900\tLast reward: -3.00\tAverage reward: -3.67\n",
      "Episode 2000\tLast reward: 44.10\tAverage reward: -21.00\n",
      "Episode 2100\tLast reward: -3.00\tAverage reward: -16.58\n",
      "Episode 2200\tLast reward: 7.10\tAverage reward: -1.62\n",
      "Episode 2300\tLast reward: -3.00\tAverage reward: -16.34\n",
      "Episode 2400\tLast reward: 27.30\tAverage reward: -22.35\n",
      "Episode 2500\tLast reward: -46.50\tAverage reward: -23.31\n",
      "Episode 2600\tLast reward: -42.20\tAverage reward: -11.80\n",
      "Episode 2700\tLast reward: -47.10\tAverage reward: -8.27\n",
      "Episode 2800\tLast reward: -3.00\tAverage reward: 7.17\n",
      "Episode 2900\tLast reward: -3.00\tAverage reward: -11.57\n",
      "Episode 3000\tLast reward: -11.30\tAverage reward: -12.76\n",
      "Episode 3100\tLast reward: 27.30\tAverage reward: -6.12\n",
      "Episode 3200\tLast reward: -12.20\tAverage reward: -7.98\n",
      "Episode 3300\tLast reward: -3.00\tAverage reward: -6.99\n",
      "Episode 3400\tLast reward: 27.30\tAverage reward: -0.19\n",
      "Episode 3500\tLast reward: 77.80\tAverage reward: -16.76\n",
      "Episode 3600\tLast reward: -31.80\tAverage reward: -14.03\n",
      "Episode 3700\tLast reward: 109.00\tAverage reward: -4.09\n",
      "Episode 3800\tLast reward: 38.60\tAverage reward: -8.55\n",
      "Episode 3900\tLast reward: -41.90\tAverage reward: -16.31\n",
      "Episode 4000\tLast reward: 48.40\tAverage reward: -4.22\n",
      "Episode 4100\tLast reward: -2.70\tAverage reward: -32.36\n",
      "Episode 4200\tLast reward: 138.40\tAverage reward: -11.70\n",
      "Episode 4300\tLast reward: -3.00\tAverage reward: 2.83\n",
      "Episode 4400\tLast reward: -42.20\tAverage reward: -5.75\n",
      "Episode 4500\tLast reward: 17.20\tAverage reward: -16.83\n",
      "Episode 4600\tLast reward: -35.50\tAverage reward: -5.86\n",
      "Episode 4700\tLast reward: 18.40\tAverage reward: -1.74\n",
      "Episode 4800\tLast reward: -21.40\tAverage reward: -19.22\n",
      "Episode 4900\tLast reward: -3.00\tAverage reward: -11.44\n",
      "Episode 5000\tLast reward: -47.10\tAverage reward: -14.03\n",
      "Episode 5100\tLast reward: -61.80\tAverage reward: -17.21\n",
      "Episode 5200\tLast reward: 57.60\tAverage reward: -15.96\n",
      "Episode 5300\tLast reward: -3.00\tAverage reward: -11.81\n",
      "Episode 5400\tLast reward: -95.80\tAverage reward: -21.48\n",
      "Episode 5500\tLast reward: -110.80\tAverage reward: 2.05\n",
      "Episode 5600\tLast reward: 17.20\tAverage reward: -9.17\n",
      "Episode 5700\tLast reward: 53.30\tAverage reward: -14.44\n",
      "Episode 5800\tLast reward: -12.50\tAverage reward: -14.91\n",
      "Episode 5900\tLast reward: 47.50\tAverage reward: -7.75\n",
      "Episode 6000\tLast reward: -3.00\tAverage reward: -32.42\n",
      "Episode 6100\tLast reward: 23.00\tAverage reward: -1.20\n",
      "Episode 6200\tLast reward: 128.30\tAverage reward: -19.91\n",
      "Episode 6300\tLast reward: -41.60\tAverage reward: -20.52\n",
      "Episode 6400\tLast reward: 77.80\tAverage reward: -1.76\n",
      "Episode 6500\tLast reward: 57.60\tAverage reward: 3.90\n",
      "Episode 6600\tLast reward: -96.10\tAverage reward: -11.10\n",
      "Episode 6700\tLast reward: -66.70\tAverage reward: -6.75\n",
      "Episode 6800\tLast reward: 57.90\tAverage reward: -4.41\n",
      "Episode 6900\tLast reward: -46.20\tAverage reward: -4.27\n",
      "Episode 7000\tLast reward: 27.30\tAverage reward: 3.10\n",
      "Episode 7100\tLast reward: 32.50\tAverage reward: -17.55\n",
      "Episode 7200\tLast reward: -61.20\tAverage reward: -13.44\n",
      "Episode 7300\tLast reward: -76.50\tAverage reward: -6.38\n",
      "Episode 7400\tLast reward: -7.90\tAverage reward: -14.95\n",
      "Episode 7500\tLast reward: -76.50\tAverage reward: -19.84\n",
      "Episode 7600\tLast reward: 77.80\tAverage reward: -7.06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-58ded91afc80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyenv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-58ded91afc80>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Don't infinite loop while learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-58ded91afc80>\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-58ded91afc80>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maffine1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(20, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 5)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + discount * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 100 == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "\n",
    "env = Myenv(my_map, rewards_map, pe, H)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
